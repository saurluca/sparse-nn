{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d32a73",
   "metadata": {},
   "source": [
    "# Hebbian Learning \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "03325cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display settings\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "# For reproducibility\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# hints: rng.multivariate_normal gives you 2-D gaussians.\n",
    "# .       quiver let's you plot actual vector arrows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bfe57e",
   "metadata": {},
   "source": [
    "## Exercise 1 • Sampling inputs & visualisation\n",
    "\n",
    "For each of the following zero‑mean 2‑D Gaussian input distributions  \n",
    "\n",
    "$$\n",
    "\\Sigma_1 =\n",
    "\\begin{pmatrix}1 & 0\\\\0 & 1\\end{pmatrix},\\quad\n",
    "\\Sigma_2 =\n",
    "\\begin{pmatrix}1 & 0.4\\\\0.4 & 1\\end{pmatrix},\\quad\n",
    "\\Sigma_3 =\n",
    "\\begin{pmatrix}1 & 0.9\\\\0.9 & 1\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "1. Draw **500 samples** and show them in a single scatter plot (use different colours/markers).  \n",
    "2. Draw one random **weight vector** $\\mathbf w\\in\\mathbb R^2$ from a standard normal distribution and add it to the scatter plot.  \n",
    "3. In a *second* panel visualise the linear activation  \n",
    "$$\n",
    "v = \\mathbf w\\cdot\\mathbf u\n",
    "$$\n",
    "on a 2‑D grid covering $[-3,3]\\times[-3,3]$ using a heatmap, together with the weight vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66525065",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# hints: rng.multivariate_normal gives you 2-D gaussians.\n",
    "#        quiver lets you plot actual vector arrows.\n",
    "\n",
    "num_samples = 500\n",
    "mean = [0, 0]\n",
    "covs = [[[1, 0], [0, 1]], [[1, 0.4], [0.4, 1]], [[1, 0.9], [0.9, 1]]]\n",
    "\n",
    "# create the 3 datasets\n",
    "datasets = []\n",
    "for distribution in covs:\n",
    "    datasets.append(rng.multivariate_normal(mean, distribution, num_samples))\n",
    "\n",
    "# plot all 3 datasets\n",
    "for i, data in enumerate(datasets):\n",
    "    plt.scatter(data[:, 0], data[:, 1], label=f\"Dataset {i + 1}\")\n",
    "\n",
    "\n",
    "# draw and plot weight vector\n",
    "weight = np.random.normal(size=2)\n",
    "weight = weight / np.linalg.norm(weight)\n",
    "\n",
    "print(weight)\n",
    "plt.quiver(0, 0, *weight, color=\"red\", angles=\"xy\", scale_units=\"xy\", scale=1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# create activations\n",
    "activations = []\n",
    "for dataset in datasets:\n",
    "    activations.append(weight * dataset)\n",
    "\n",
    "# plot all 3 activations\n",
    "for i, data in enumerate(activations):\n",
    "    plt.scatter(data[:, 0], data[:, 1], label=f\"Dataset {i + 1}\")\n",
    "\n",
    "plt.legend()\n",
    "# create grid for heatmap\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "grid_points = np.column_stack((X.ravel(), Y.ravel()))\n",
    "\n",
    "# calculate activation for each point\n",
    "activations = np.dot(grid_points, weight)\n",
    "activations = activations.reshape(X.shape)\n",
    "\n",
    "# plot heatmap\n",
    "plt.figure()\n",
    "plt.imshow(activations, extent=[-3, 3, -3, 3], origin=\"lower\", aspect=\"equal\")\n",
    "plt.colorbar(label=\"Activation\")\n",
    "\n",
    "# plot weight vector on heatmap\n",
    "plt.quiver(0, 0, *weight, color=\"red\", angles=\"xy\", scale_units=\"xy\", scale=1)\n",
    "plt.title(\"Linear Activation\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e24d03a",
   "metadata": {},
   "source": [
    "## Exercise 2 • Dynamics of Hebbian plasticity\n",
    "\n",
    "### 2 a — Simple Hebb rule  \n",
    "Implement  Hebbian learning\n",
    "\n",
    "\\begin{align*}\n",
    "\\Delta\\mathbf w_n &= \\,v\\,\\mathbf u \\\\\n",
    "v_n &= \\mathbf w_n^\\top\\mathbf u \\\\\n",
    "\\mathbf w_{n+1} &= w + \\eta \\Delta\\mathbf w_n\n",
    "\\end{align*}\n",
    "\n",
    "* Use a learning rate $\\eta=10^{-3}$.  \n",
    "* Start from a different random initial vector $\\mathbf w_0$.  \n",
    "* Update once per randomly drawn input sample (2000 steps should suffice).  \n",
    "* **Plot** the trajectory of the weight vector on top of the corresponding input scatter\n",
    "  for **each** of the three input distributions\n",
    "  (use a marker for every 10 updates to keep the plot readable).\n",
    "\n",
    "Describe qualitatively what you observe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6dcd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hebb_update(w, u, lr):\n",
    "    v = w * u\n",
    "    grad = v * u\n",
    "    return w + lr * grad\n",
    "\n",
    "\n",
    "# you can use this function for other learning rules by changing update_fn. cov determines inputs.\n",
    "def run_learning(update_fn, cov, lr=1e-3, steps=2000):\n",
    "    # initialize weights, draw a data point, update weights. save weights to a vector.\n",
    "    weights = np.random.normal(size=2)\n",
    "    # normalize initial weights\n",
    "    weights = weights / np.linalg.norm(weights)\n",
    "    traj = []\n",
    "    for i in range(steps):\n",
    "        data = rng.multivariate_normal([0, 0], cov)\n",
    "        weights = update_fn(weights, data, lr)\n",
    "        if i % 10 == 0:\n",
    "            traj.append(weights.copy())\n",
    "    return traj\n",
    "\n",
    "\n",
    "# it's helpful to standardize your plotting for the rest of the experiments.\n",
    "# (ax=None lets you pass in subfigure axis)\n",
    "def plot_trajectory(traj, data, title, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.scatter(data[:, 0], data[:, 1], alpha=0.2, c=\"gray\", label=\"Data\")\n",
    "\n",
    "    # Plot trajectory as connected line segments\n",
    "    traj = np.array(traj)\n",
    "    ax.plot(traj[:, 0], traj[:, 1], alpha=0.5, label=\"Trajectory\", linewidth=4)\n",
    "\n",
    "    # Plot from the origin to the last trajectory point\n",
    "    last_traj = traj[-1]\n",
    "    ax.quiver(\n",
    "        0, 0, last_traj[0], last_traj[1], color=\"red\", scale=10, label=\"Last Trajectory\"\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(-4, 4)\n",
    "    ax.set_ylim(-4, 4)\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "# run the experiment for each cov. matrix and produce the plot for each condition.\n",
    "for i in range(3):\n",
    "    traj = run_learning(hebb_update, covs[i])\n",
    "    plot_trajectory(traj, datasets[i], f\"Weight trajectory for distribution {i + 1}\")\n",
    "    \n",
    "    # implement new plasticity update\n",
    "def hebb_clip_update(w, u, lr, w_max=2):\n",
    "    v = w * u\n",
    "    grad = v * u\n",
    "    updated_w = w + lr * grad\n",
    "    return updated_w / np.maximum(1, np.linalg.norm(updated_w) / w_max)\n",
    "\n",
    "\n",
    "# repeat experiment (use run_learning and plot_trajectory)\n",
    "# run the experiment for each cov. matrix and produce the plot for each condition.\n",
    "for i in range(3):\n",
    "    weight_history = run_learning(hebb_clip_update, covs[i])\n",
    "    plot_trajectory(weight_history, datasets[i], \" sample title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851285b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b8d7bd1",
   "metadata": {},
   "source": [
    "### 2 b — Catch your weight vectors! \n",
    "Repeat part (a) but clip the weight vector to a fixed maximum length  \n",
    "$\\lVert\\mathbf w\\rVert \\le w_\\max = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3675832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement new plasticity update\n",
    "def hebb_clip_update(w, u, lr, w_max=2):\n",
    "    v = w * u\n",
    "    grad = v * u\n",
    "    updated_w = w + lr * grad\n",
    "    return updated_w / np.maximum(1, np.linalg.norm(updated_w) / w_max)\n",
    "\n",
    "\n",
    "# repeat experiment (use run_learning and plot_trajectory)\n",
    "# run the experiment for each cov. matrix and produce the plot for each condition.\n",
    "for i in range(3):\n",
    "    weight_history = run_learning(hebb_clip_update, covs[i])\n",
    "    plot_trajectory(weight_history, datasets[i], \" sample title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d8ad1e",
   "metadata": {},
   "source": [
    "### 2 c — Oja’s rule  \n",
    "Implement Oja’s *normalising* rule\n",
    "\n",
    "$$\n",
    "\\Delta\\mathbf w = \\eta\\bigl(v\\,\\mathbf u - \\alpha\\,v^{2}\\,\\mathbf w\\bigr)\n",
    "$$\n",
    "\n",
    "with $\\alpha=1$.  \n",
    "Repeat the experiment for each distribution and verify that the norm of \\(\\mathbf w\\) converges.  \n",
    "Show analytically that at equilibrium  \n",
    "\n",
    "$$\n",
    "\\bigl\\lVert\\mathbf w\\bigr\\rVert = \\frac{1}{\\sqrt\\alpha}.\n",
    "$$\n",
    "\n",
    "*(Hint: take the derivative of $\\lVert\\mathbf w\\rVert^{2}$ and set it to zero).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6c8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement new plasticity update\n",
    "def oja_update(w, u, lr, alpha=1.0):\n",
    "    v = w @ u\n",
    "    return w + lr * (v * u - alpha * v**2 * w)\n",
    "\n",
    "\n",
    "final_norms = []\n",
    "\n",
    "for i in range(3):\n",
    "    traj = run_learning(oja_update, covs[i])\n",
    "    plot_trajectory(traj, datasets[i], \" sample title\")\n",
    "    final_norms.append(np.linalg.norm(traj[-1]))\n",
    "\n",
    "# repeat experiment (use run_learning and plot_trajectory)\n",
    "print(\"Final weight norms/lengths:\", np.round(final_norms, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc32d2",
   "metadata": {},
   "source": [
    "## Exercise 3 • Convergence to the First Principal Component\n",
    "\n",
    "Using the simulations from Exercise 2, confirm *empirically* that the final weight\n",
    "vector aligns with the **first principal component** (eigenvector with the\n",
    "largest eigenvalue) of the input covariance matrix.\n",
    "\n",
    "1. Compute the covariance matrix of each generated data set and its principal\n",
    "   eigenvector $\\mathbf e_1$.\n",
    "2. Compute the cosine similarity the final weight vector $\\mathbf w_*$ (from\n",
    "   Oja’s rule) and $\\mathbf e_1$.\n",
    "3. Plot the two vectors in the same figure and report the angle in degrees.\n",
    "\n",
    "*(A small angle ≈ 0 shows successful convergence.)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee87c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute eigenvectors, pick the biggets one (np.linalg!)\n",
    "# INSERT_YOUR_REWRITE_HERE\n",
    "def principal_eigenvector(cov):\n",
    "    eigvals, eigvecs = np.linalg.eig(cov)\n",
    "    return eigvecs[:, np.argmax(eigvals)]\n",
    "\n",
    "\n",
    "# compute the cosine similarity betwee a and b\n",
    "def angle_deg(a, b):\n",
    "    return np.degrees(np.arccos(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))))\n",
    "\n",
    "\n",
    "# plot the trajectory of the weight vector and the principal component for all 3 distributions\n",
    "for i in range(3):\n",
    "    traj = run_learning(oja_update, covs[i])\n",
    "    final_weight = traj[-1]\n",
    "    principal_component = principal_eigenvector(covs[i])\n",
    "    angle_diff = angle_deg(final_weight, principal_component)\n",
    "    plot_trajectory(traj, datasets[i], \" sample title\")\n",
    "    plt.quiver(\n",
    "        0,\n",
    "        0,\n",
    "        principal_component[0],\n",
    "        principal_component[1],\n",
    "        color=\"blue\",\n",
    "        label=\"First Principal Component\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.title(f\"Angle Difference: {angle_diff:.2f} degrees\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
